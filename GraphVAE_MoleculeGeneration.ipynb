{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOBSope0n7hNh1PJE+K0Lk3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmicoBinsfinder/GNN_Viscosity_Prediction/blob/main/GraphVAE_MoleculeGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swyIxixYlqUC",
        "outputId": "f6498ee7-24da-4bcb-9b0d-589e80bca1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install rdkit-pypi==2021.9.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from rdkit import Chem, RDLogger\n",
        "from rdkit.Chem import BondType\n",
        "from rdkit.Chem.Draw import MolsToGridImage\n",
        "\n",
        "RDLogger.DisableLog(\"rdApp.*\")"
      ],
      "metadata": {
        "id": "GsTpdfXglyYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = keras.utils.get_file(\n",
        "    \"/content/250k_rndm_zinc_drugs_clean_3.csv\",\n",
        "    \"https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\",\n",
        ")\n",
        "\n",
        "df = pd.read_csv(\"/content/Dataset.csv\")\n",
        "df[\"smiles\"] = df[\"smiles\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "snRKB-Lgl0ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SMILE_CHARSET = '[\"C\", \"B\", \"F\", \"I\", \"H\", \"O\", \"N\", \"S\", \"P\", \"Cl\", \"Br\"]'\n",
        "\n",
        "bond_mapping = {\"SINGLE\": 0, \"DOUBLE\": 1, \"TRIPLE\": 2, \"AROMATIC\": 3}\n",
        "bond_mapping.update(\n",
        "    {0: BondType.SINGLE, 1: BondType.DOUBLE, 2: BondType.TRIPLE, 3: BondType.AROMATIC}\n",
        ")\n",
        "SMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)\n",
        "\n",
        "MAX_MOLSIZE = max(df[\"smiles\"].str.len())\n",
        "SMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))\n",
        "index_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))\n",
        "atom_mapping = dict(SMILE_to_index)\n",
        "atom_mapping.update(index_to_SMILE)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 10\n",
        "\n",
        "VAE_LR = 5e-4\n",
        "NUM_ATOMS = 120  # Maximum number of atoms\n",
        "\n",
        "ATOM_DIM = len(SMILE_CHARSET)  # Number of atom types\n",
        "BOND_DIM = 4 + 1  # Number of bond types\n",
        "LATENT_DIM = 435  # Size of the latent space\n",
        "\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    # Converts SMILES to molecule object\n",
        "    molecule = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # Initialize adjacency and feature tensor\n",
        "    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), \"float32\")\n",
        "    features = np.zeros((NUM_ATOMS, ATOM_DIM), \"float32\")\n",
        "\n",
        "    # loop over each atom in molecule\n",
        "    for atom in molecule.GetAtoms():\n",
        "        i = atom.GetIdx()\n",
        "        atom_type = atom_mapping[atom.GetSymbol()]\n",
        "        features[i] = np.eye(ATOM_DIM)[atom_type]\n",
        "        # loop over one-hop neighbors\n",
        "        for neighbor in atom.GetNeighbors():\n",
        "            j = neighbor.GetIdx()\n",
        "            bond = molecule.GetBondBetweenAtoms(i, j)\n",
        "            bond_type_idx = bond_mapping[bond.GetBondType().name]\n",
        "            adjacency[bond_type_idx, [i, j], [j, i]] = 1\n",
        "\n",
        "    # Where no bond, add 1 to last channel (indicating \"non-bond\")\n",
        "    # Notice: channels-first\n",
        "    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1\n",
        "\n",
        "    # Where no atom, add 1 to last column (indicating \"non-atom\")\n",
        "    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1\n",
        "\n",
        "    return adjacency, features\n",
        "\n",
        "\n",
        "def graph_to_molecule(graph):\n",
        "    # Unpack graph\n",
        "    adjacency, features = graph\n",
        "\n",
        "    # RWMol is a molecule object intended to be edited\n",
        "    molecule = Chem.RWMol()\n",
        "\n",
        "    # Remove \"no atoms\" & atoms with no bonds\n",
        "    keep_idx = np.where(\n",
        "        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n",
        "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n",
        "    )[0]\n",
        "    features = features[keep_idx]\n",
        "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
        "\n",
        "    # Add atoms to molecule\n",
        "    for atom_type_idx in np.argmax(features, axis=1):\n",
        "        atom = Chem.Atom(atom_mapping[atom_type_idx])\n",
        "        _ = molecule.AddAtom(atom)\n",
        "\n",
        "    # Add bonds between atoms in molecule; based on the upper triangles\n",
        "    # of the [symmetric] adjacency tensor\n",
        "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
        "    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):\n",
        "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
        "            continue\n",
        "        bond_type = bond_mapping[bond_ij]\n",
        "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
        "\n",
        "    # Sanitize the molecule; for more information on sanitization, see\n",
        "    # https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization\n",
        "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
        "    # Let's be strict. If sanitization fails, return None\n",
        "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
        "        return None\n",
        "\n",
        "    return molecule"
      ],
      "metadata": {
        "id": "FcSJif59l17r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df.sample(frac=0.75, random_state=42)  # random state is a seed value\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "adjacency_tensor, feature_tensor, qed_tensor = [], [], []\n",
        "for idx in range(8000):\n",
        "    adjacency, features = smiles_to_graph(train_df.loc[idx][\"smiles\"])\n",
        "    qed = train_df.loc[idx][\"VI\"]\n",
        "    adjacency_tensor.append(adjacency)\n",
        "    feature_tensor.append(features)\n",
        "    qed_tensor.append(qed)\n",
        "\n",
        "adjacency_tensor = np.array(adjacency_tensor)\n",
        "feature_tensor = np.array(feature_tensor)\n",
        "qed_tensor = np.array(qed_tensor)\n",
        "\n",
        "\n",
        "class RelationalGraphConvLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        units=128,\n",
        "        activation=\"relu\",\n",
        "        use_bias=False,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        bond_dim = input_shape[0][1]\n",
        "        atom_dim = input_shape[1][2]\n",
        "\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(bond_dim, atom_dim, self.units),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            trainable=True,\n",
        "            name=\"W\",\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                shape=(bond_dim, 1, self.units),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                trainable=True,\n",
        "                name=\"b\",\n",
        "                dtype=tf.float32,\n",
        "            )\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        adjacency, features = inputs\n",
        "        # Aggregate information from neighbors\n",
        "        x = tf.matmul(adjacency, features[:, None, :, :])\n",
        "        # Apply linear transformation\n",
        "        x = tf.matmul(x, self.kernel)\n",
        "        if self.use_bias:\n",
        "            x += self.bias\n",
        "        # Reduce bond types dim\n",
        "        x_reduced = tf.reduce_sum(x, axis=1)\n",
        "        # Apply non-linear transformation\n",
        "        return self.activation(x_reduced)"
      ],
      "metadata": {
        "id": "H34klQfTl56W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_encoder(\n",
        "    gconv_units, latent_dim, adjacency_shape, feature_shape, dense_units, dropout_rate\n",
        "):\n",
        "    adjacency = keras.layers.Input(shape=adjacency_shape)\n",
        "    features = keras.layers.Input(shape=feature_shape)\n",
        "\n",
        "    # Propagate through one or more graph convolutional layers\n",
        "    features_transformed = features\n",
        "    for units in gconv_units:\n",
        "        features_transformed = RelationalGraphConvLayer(units)(\n",
        "            [adjacency, features_transformed]\n",
        "        )\n",
        "    # Reduce 2-D representation of molecule to 1-D\n",
        "    x = keras.layers.GlobalAveragePooling1D()(features_transformed)\n",
        "\n",
        "    # Propagate through one or more densely connected layers\n",
        "    for units in dense_units:\n",
        "        x = layers.Dense(units, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    z_mean = layers.Dense(latent_dim, dtype=\"float32\", name=\"z_mean\")(x)\n",
        "    log_var = layers.Dense(latent_dim, dtype=\"float32\", name=\"log_var\")(x)\n",
        "\n",
        "    encoder = keras.Model([adjacency, features], [z_mean, log_var], name=\"encoder\")\n",
        "\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def get_decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape):\n",
        "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "\n",
        "    x = latent_inputs\n",
        "    for units in dense_units:\n",
        "        x = keras.layers.Dense(units, activation=\"tanh\")(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)\n",
        "    x_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(x)\n",
        "    x_adjacency = keras.layers.Reshape(adjacency_shape)(x_adjacency)\n",
        "    # Symmetrify tensors in the last two dimensions\n",
        "    x_adjacency = (x_adjacency + tf.transpose(x_adjacency, (0, 1, 3, 2))) / 2\n",
        "    x_adjacency = keras.layers.Softmax(axis=1)(x_adjacency)\n",
        "\n",
        "    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)\n",
        "    x_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(x)\n",
        "    x_features = keras.layers.Reshape(feature_shape)(x_features)\n",
        "    x_features = keras.layers.Softmax(axis=2)(x_features)\n",
        "\n",
        "    decoder = keras.Model(\n",
        "        latent_inputs, outputs=[x_adjacency, x_features], name=\"decoder\"\n",
        "    )\n",
        "\n",
        "    return decoder"
      ],
      "metadata": {
        "id": "OzPlP2LPl8Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_log_var)[0]\n",
        "        dim = tf.shape(z_log_var)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "1EUaN1aPl-qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MoleculeGenerator(keras.Model):\n",
        "    def __init__(self, encoder, decoder, max_len, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.property_prediction_layer = layers.Dense(1)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.train_total_loss_tracker = keras.metrics.Mean(name=\"train_total_loss\")\n",
        "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_total_loss\")\n",
        "\n",
        "    def train_step(self, data):\n",
        "        adjacency_tensor, feature_tensor, qed_tensor = data[0]\n",
        "        graph_real = [adjacency_tensor, feature_tensor]\n",
        "        self.batch_size = tf.shape(qed_tensor)[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, qed_pred, gen_adjacency, gen_features = self(\n",
        "                graph_real, training=True\n",
        "            )\n",
        "            graph_generated = [gen_adjacency, gen_features]\n",
        "            total_loss = self._compute_loss(\n",
        "                z_log_var, z_mean, qed_tensor, qed_pred, graph_real, graph_generated\n",
        "            )\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        self.train_total_loss_tracker.update_state(total_loss)\n",
        "        return {\"loss\": self.train_total_loss_tracker.result()}\n",
        "\n",
        "    def _compute_loss(\n",
        "        self, z_log_var, z_mean, qed_true, qed_pred, graph_real, graph_generated\n",
        "    ):\n",
        "\n",
        "        adjacency_real, features_real = graph_real\n",
        "        adjacency_gen, features_gen = graph_generated\n",
        "\n",
        "        adjacency_loss = tf.reduce_mean(\n",
        "            tf.reduce_sum(\n",
        "                keras.losses.categorical_crossentropy(adjacency_real, adjacency_gen),\n",
        "                axis=(1, 2),\n",
        "            )\n",
        "        )\n",
        "        features_loss = tf.reduce_mean(\n",
        "            tf.reduce_sum(\n",
        "                keras.losses.categorical_crossentropy(features_real, features_gen),\n",
        "                axis=(1),\n",
        "            )\n",
        "        )\n",
        "        kl_loss = -0.5 * tf.reduce_sum(\n",
        "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), 1\n",
        "        )\n",
        "        kl_loss = tf.reduce_mean(kl_loss)\n",
        "\n",
        "        property_loss = tf.reduce_mean(\n",
        "            keras.losses.binary_crossentropy(qed_true, qed_pred)\n",
        "        )\n",
        "\n",
        "        graph_loss = self._gradient_penalty(graph_real, graph_generated)\n",
        "\n",
        "        return kl_loss + property_loss + graph_loss + adjacency_loss + features_loss\n",
        "\n",
        "    def _gradient_penalty(self, graph_real, graph_generated):\n",
        "        # Unpack graphs\n",
        "        adjacency_real, features_real = graph_real\n",
        "        adjacency_generated, features_generated = graph_generated\n",
        "\n",
        "        # Generate interpolated graphs (adjacency_interp and features_interp)\n",
        "        alpha = tf.random.uniform([self.batch_size])\n",
        "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1, 1))\n",
        "        adjacency_interp = (adjacency_real * alpha) + (1 - alpha) * adjacency_generated\n",
        "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1))\n",
        "        features_interp = (features_real * alpha) + (1 - alpha) * features_generated\n",
        "\n",
        "        # Compute the logits of interpolated graphs\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(adjacency_interp)\n",
        "            tape.watch(features_interp)\n",
        "            _, _, logits, _, _ = self(\n",
        "                [adjacency_interp, features_interp], training=True\n",
        "            )\n",
        "\n",
        "        # Compute the gradients with respect to the interpolated graphs\n",
        "        grads = tape.gradient(logits, [adjacency_interp, features_interp])\n",
        "        # Compute the gradient penalty\n",
        "        grads_adjacency_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2\n",
        "        grads_features_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2\n",
        "        return tf.reduce_mean(\n",
        "            tf.reduce_mean(grads_adjacency_penalty, axis=(-2, -1))\n",
        "            + tf.reduce_mean(grads_features_penalty, axis=(-1))\n",
        "        )\n",
        "\n",
        "    def inference(self, batch_size):\n",
        "        z = tf.random.normal((batch_size, LATENT_DIM))\n",
        "        reconstruction_adjacency, reconstruction_features = model.decoder.predict(z)\n",
        "        # obtain one-hot encoded adjacency tensor\n",
        "        adjacency = tf.argmax(reconstruction_adjacency, axis=1)\n",
        "        adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)\n",
        "        # Remove potential self-loops from adjacency\n",
        "        adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))\n",
        "        # obtain one-hot encoded feature tensor\n",
        "        features = tf.argmax(reconstruction_features, axis=2)\n",
        "        features = tf.one_hot(features, depth=ATOM_DIM, axis=2)\n",
        "        return [\n",
        "            graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n",
        "            for i in range(batch_size)\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, log_var = self.encoder(inputs)\n",
        "        z = Sampling()([z_mean, log_var])\n",
        "\n",
        "        gen_adjacency, gen_features = self.decoder(z)\n",
        "\n",
        "        property_pred = self.property_prediction_layer(z_mean)\n",
        "\n",
        "        return z_mean, log_var, property_pred, gen_adjacency, gen_features"
      ],
      "metadata": {
        "id": "GQsbOnTamAYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae_optimizer = tf.keras.optimizers.Adam(learning_rate=VAE_LR)\n",
        "\n",
        "encoder = get_encoder(\n",
        "    gconv_units=[9],\n",
        "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
        "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
        "    latent_dim=LATENT_DIM,\n",
        "    dense_units=[512],\n",
        "    dropout_rate=0.0,\n",
        ")\n",
        "decoder = get_decoder(\n",
        "    dense_units=[128, 256, 512],\n",
        "    dropout_rate=0.2,\n",
        "    latent_dim=LATENT_DIM,\n",
        "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
        "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
        ")\n",
        "\n",
        "model = MoleculeGenerator(encoder, decoder, MAX_MOLSIZE)\n",
        "\n",
        "model.compile(vae_optimizer)\n",
        "history = model.fit([adjacency_tensor, feature_tensor, qed_tensor], epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "YUHCyBJTmCrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "molecules = model.inference(1000)\n",
        "\n",
        "MolsToGridImage(\n",
        "    [m for m in molecules if m is not None][:1000], molsPerRow=5, subImgSize=(260, 160)\n",
        ")"
      ],
      "metadata": {
        "id": "5osULHLRmEBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}