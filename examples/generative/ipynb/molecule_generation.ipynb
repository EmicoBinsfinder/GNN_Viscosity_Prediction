{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r4Xgr1plVKG"
      },
      "outputs": [],
      "source": [
        "!pip -q install rdkit-pypi==2021.9.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_fEACKAlVKH"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from rdkit import Chem, RDLogger\n",
        "from rdkit.Chem import BondType\n",
        "from rdkit.Chem.Draw import MolsToGridImage\n",
        "\n",
        "RDLogger.DisableLog(\"rdApp.*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAsau8TQlVKI"
      },
      "outputs": [],
      "source": [
        "csv_path = keras.utils.get_file(\n",
        "    \"/content/250k_rndm_zinc_drugs_clean_3.csv\",\n",
        "    \"https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\",\n",
        ")\n",
        "\n",
        "df = pd.read_csv(\"/content/250k_rndm_zinc_drugs_clean_3.csv\")\n",
        "df[\"smiles\"] = df[\"smiles\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDK8v1mdlVKI"
      },
      "outputs": [],
      "source": [
        "SMILE_CHARSET = '[\"C\", \"B\", \"F\", \"I\", \"H\", \"O\", \"N\", \"S\", \"P\", \"Cl\", \"Br\"]'\n",
        "\n",
        "bond_mapping = {\"SINGLE\": 0, \"DOUBLE\": 1, \"TRIPLE\": 2, \"AROMATIC\": 3}\n",
        "bond_mapping.update(\n",
        "    {0: BondType.SINGLE, 1: BondType.DOUBLE, 2: BondType.TRIPLE, 3: BondType.AROMATIC}\n",
        ")\n",
        "SMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)\n",
        "\n",
        "MAX_MOLSIZE = max(df[\"smiles\"].str.len())\n",
        "SMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))\n",
        "index_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))\n",
        "atom_mapping = dict(SMILE_to_index)\n",
        "atom_mapping.update(index_to_SMILE)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 10\n",
        "\n",
        "VAE_LR = 5e-4\n",
        "NUM_ATOMS = 120  # Maximum number of atoms\n",
        "\n",
        "ATOM_DIM = len(SMILE_CHARSET)  # Number of atom types\n",
        "BOND_DIM = 4 + 1  # Number of bond types\n",
        "LATENT_DIM = 435  # Size of the latent space\n",
        "\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    # Converts SMILES to molecule object\n",
        "    molecule = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # Initialize adjacency and feature tensor\n",
        "    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), \"float32\")\n",
        "    features = np.zeros((NUM_ATOMS, ATOM_DIM), \"float32\")\n",
        "\n",
        "    # loop over each atom in molecule\n",
        "    for atom in molecule.GetAtoms():\n",
        "        i = atom.GetIdx()\n",
        "        atom_type = atom_mapping[atom.GetSymbol()]\n",
        "        features[i] = np.eye(ATOM_DIM)[atom_type]\n",
        "        # loop over one-hop neighbors\n",
        "        for neighbor in atom.GetNeighbors():\n",
        "            j = neighbor.GetIdx()\n",
        "            bond = molecule.GetBondBetweenAtoms(i, j)\n",
        "            bond_type_idx = bond_mapping[bond.GetBondType().name]\n",
        "            adjacency[bond_type_idx, [i, j], [j, i]] = 1\n",
        "\n",
        "    # Where no bond, add 1 to last channel (indicating \"non-bond\")\n",
        "    # Notice: channels-first\n",
        "    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1\n",
        "\n",
        "    # Where no atom, add 1 to last column (indicating \"non-atom\")\n",
        "    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1\n",
        "\n",
        "    return adjacency, features\n",
        "\n",
        "\n",
        "def graph_to_molecule(graph):\n",
        "    # Unpack graph\n",
        "    adjacency, features = graph\n",
        "\n",
        "    # RWMol is a molecule object intended to be edited\n",
        "    molecule = Chem.RWMol()\n",
        "\n",
        "    # Remove \"no atoms\" & atoms with no bonds\n",
        "    keep_idx = np.where(\n",
        "        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n",
        "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n",
        "    )[0]\n",
        "    features = features[keep_idx]\n",
        "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
        "\n",
        "    # Add atoms to molecule\n",
        "    for atom_type_idx in np.argmax(features, axis=1):\n",
        "        atom = Chem.Atom(atom_mapping[atom_type_idx])\n",
        "        _ = molecule.AddAtom(atom)\n",
        "\n",
        "    # Add bonds between atoms in molecule; based on the upper triangles\n",
        "    # of the [symmetric] adjacency tensor\n",
        "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
        "    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):\n",
        "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
        "            continue\n",
        "        bond_type = bond_mapping[bond_ij]\n",
        "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
        "\n",
        "    # Sanitize the molecule; for more information on sanitization, see\n",
        "    # https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization\n",
        "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
        "    # Let's be strict. If sanitization fails, return None\n",
        "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
        "        return None\n",
        "\n",
        "    return molecule\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0b2tbsclVKJ"
      },
      "source": [
        "##  Generate training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syLXK-volVKJ"
      },
      "outputs": [],
      "source": [
        "train_df = df.sample(frac=0.75, random_state=42)  # random state is a seed value\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "adjacency_tensor, feature_tensor, qed_tensor = [], [], []\n",
        "for idx in range(8000):\n",
        "    adjacency, features = smiles_to_graph(train_df.loc[idx][\"smiles\"])\n",
        "    qed = train_df.loc[idx][\"qed\"]\n",
        "    adjacency_tensor.append(adjacency)\n",
        "    feature_tensor.append(features)\n",
        "    qed_tensor.append(qed)\n",
        "\n",
        "adjacency_tensor = np.array(adjacency_tensor)\n",
        "feature_tensor = np.array(feature_tensor)\n",
        "qed_tensor = np.array(qed_tensor)\n",
        "\n",
        "\n",
        "class RelationalGraphConvLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        units=128,\n",
        "        activation=\"relu\",\n",
        "        use_bias=False,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        bond_dim = input_shape[0][1]\n",
        "        atom_dim = input_shape[1][2]\n",
        "\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(bond_dim, atom_dim, self.units),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            trainable=True,\n",
        "            name=\"W\",\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                shape=(bond_dim, 1, self.units),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                trainable=True,\n",
        "                name=\"b\",\n",
        "                dtype=tf.float32,\n",
        "            )\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        adjacency, features = inputs\n",
        "        # Aggregate information from neighbors\n",
        "        x = tf.matmul(adjacency, features[:, None, :, :])\n",
        "        # Apply linear transformation\n",
        "        x = tf.matmul(x, self.kernel)\n",
        "        if self.use_bias:\n",
        "            x += self.bias\n",
        "        # Reduce bond types dim\n",
        "        x_reduced = tf.reduce_sum(x, axis=1)\n",
        "        # Apply non-linear transformation\n",
        "        return self.activation(x_reduced)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snjJx-kblVKK"
      },
      "source": [
        "## Build the Encoder and Decoder\n",
        "\n",
        "The Encoder takes as input a molecule's graph adjacency matrix and feature matrix.\n",
        "These features are processed via a Graph Convolution layer, then are flattened and\n",
        "processed by several Dense layers to derive `z_mean` and `log_var`, the\n",
        "latent-space representation of the molecule.\n",
        "\n",
        "**Graph Convolution layer**: The relational graph convolution layer implements\n",
        "non-linearly transformed neighbourhood aggregations. We can define these layers as\n",
        "follows:\n",
        "\n",
        "`H_hat**(l+1) = σ(D_hat**(-1) * A_hat * H_hat**(l+1) * W**(l))`\n",
        "\n",
        "Where `σ` denotes the non-linear transformation (commonly a ReLU activation), `A` the\n",
        "adjacency tensor, `H_hat**(l)` the feature tensor at the `l-th` layer, `D_hat**(-1)` the\n",
        "inverse diagonal degree tensor of `A_hat`, and `W_hat**(l)` the trainable weight tensor\n",
        "at the `l-th` layer. Specifically, for each bond type (relation), the degree tensor\n",
        "expresses, in the diagonal, the number of bonds attached to each atom.\n",
        "\n",
        "Source:\n",
        "[WGAN-GP with R-GCN for the generation of small molecular graphs](https://keras.io/examples/generative/wgan-graphs/))\n",
        "\n",
        "The Decoder takes as input the latent-space representation and predicts\n",
        "the graph adjacency matrix and feature matrix of the corresponding molecules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvJ5wZbklVKK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_encoder(\n",
        "    gconv_units, latent_dim, adjacency_shape, feature_shape, dense_units, dropout_rate\n",
        "):\n",
        "    adjacency = keras.layers.Input(shape=adjacency_shape)\n",
        "    features = keras.layers.Input(shape=feature_shape)\n",
        "\n",
        "    # Propagate through one or more graph convolutional layers\n",
        "    features_transformed = features\n",
        "    for units in gconv_units:\n",
        "        features_transformed = RelationalGraphConvLayer(units)(\n",
        "            [adjacency, features_transformed]\n",
        "        )\n",
        "    # Reduce 2-D representation of molecule to 1-D\n",
        "    x = keras.layers.GlobalAveragePooling1D()(features_transformed)\n",
        "\n",
        "    # Propagate through one or more densely connected layers\n",
        "    for units in dense_units:\n",
        "        x = layers.Dense(units, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    z_mean = layers.Dense(latent_dim, dtype=\"float32\", name=\"z_mean\")(x)\n",
        "    log_var = layers.Dense(latent_dim, dtype=\"float32\", name=\"log_var\")(x)\n",
        "\n",
        "    encoder = keras.Model([adjacency, features], [z_mean, log_var], name=\"encoder\")\n",
        "\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def get_decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape):\n",
        "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "\n",
        "    x = latent_inputs\n",
        "    for units in dense_units:\n",
        "        x = keras.layers.Dense(units, activation=\"tanh\")(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)\n",
        "    x_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(x)\n",
        "    x_adjacency = keras.layers.Reshape(adjacency_shape)(x_adjacency)\n",
        "    # Symmetrify tensors in the last two dimensions\n",
        "    x_adjacency = (x_adjacency + tf.transpose(x_adjacency, (0, 1, 3, 2))) / 2\n",
        "    x_adjacency = keras.layers.Softmax(axis=1)(x_adjacency)\n",
        "\n",
        "    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)\n",
        "    x_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(x)\n",
        "    x_features = keras.layers.Reshape(feature_shape)(x_features)\n",
        "    x_features = keras.layers.Softmax(axis=2)(x_features)\n",
        "\n",
        "    decoder = keras.Model(\n",
        "        latent_inputs, outputs=[x_adjacency, x_features], name=\"decoder\"\n",
        "    )\n",
        "\n",
        "    return decoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ9UXfrOlVKK"
      },
      "source": [
        "## Build the Sampling layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWJpDW4jlVKK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_log_var)[0]\n",
        "        dim = tf.shape(z_log_var)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf8KR4q9lVKK"
      },
      "source": [
        "## Build the VAE\n",
        "\n",
        "This model is trained to optimize four losses:\n",
        "\n",
        "* Categorical crossentropy\n",
        "* KL divergence loss\n",
        "* Property prediction loss\n",
        "* Graph loss (gradient penalty)\n",
        "\n",
        "The categorical crossentropy loss function measures the model's\n",
        "reconstruction accuracy. The Property prediction loss estimates the mean squared\n",
        "error between predicted and actual properties after running the latent representation\n",
        "through a property prediction model. The property\n",
        "prediction of the model is optimized via binary crossentropy. The gradient\n",
        "penalty is further guided by the model's property (QED) prediction.\n",
        "\n",
        "A gradient penalty is an alternative soft constraint on the\n",
        "1-Lipschitz continuity as an improvement upon the gradient clipping scheme from the\n",
        "original neural network\n",
        "(\"1-Lipschitz continuity\" means that the norm of the gradient is at most 1 at evey single\n",
        "point of the function).\n",
        "It adds a regularization term to the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgQDyXaElVKK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MoleculeGenerator(keras.Model):\n",
        "    def __init__(self, encoder, decoder, max_len, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.property_prediction_layer = layers.Dense(1)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.train_total_loss_tracker = keras.metrics.Mean(name=\"train_total_loss\")\n",
        "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_total_loss\")\n",
        "\n",
        "    def train_step(self, data):\n",
        "        adjacency_tensor, feature_tensor, qed_tensor = data[0]\n",
        "        graph_real = [adjacency_tensor, feature_tensor]\n",
        "        self.batch_size = tf.shape(qed_tensor)[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, qed_pred, gen_adjacency, gen_features = self(\n",
        "                graph_real, training=True\n",
        "            )\n",
        "            graph_generated = [gen_adjacency, gen_features]\n",
        "            total_loss = self._compute_loss(\n",
        "                z_log_var, z_mean, qed_tensor, qed_pred, graph_real, graph_generated\n",
        "            )\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        self.train_total_loss_tracker.update_state(total_loss)\n",
        "        return {\"loss\": self.train_total_loss_tracker.result()}\n",
        "\n",
        "    def _compute_loss(\n",
        "        self, z_log_var, z_mean, qed_true, qed_pred, graph_real, graph_generated\n",
        "    ):\n",
        "\n",
        "        adjacency_real, features_real = graph_real\n",
        "        adjacency_gen, features_gen = graph_generated\n",
        "\n",
        "        adjacency_loss = tf.reduce_mean(\n",
        "            tf.reduce_sum(\n",
        "                keras.losses.categorical_crossentropy(adjacency_real, adjacency_gen),\n",
        "                axis=(1, 2),\n",
        "            )\n",
        "        )\n",
        "        features_loss = tf.reduce_mean(\n",
        "            tf.reduce_sum(\n",
        "                keras.losses.categorical_crossentropy(features_real, features_gen),\n",
        "                axis=(1),\n",
        "            )\n",
        "        )\n",
        "        kl_loss = -0.5 * tf.reduce_sum(\n",
        "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), 1\n",
        "        )\n",
        "        kl_loss = tf.reduce_mean(kl_loss)\n",
        "\n",
        "        property_loss = tf.reduce_mean(\n",
        "            keras.losses.binary_crossentropy(qed_true, qed_pred)\n",
        "        )\n",
        "\n",
        "        graph_loss = self._gradient_penalty(graph_real, graph_generated)\n",
        "\n",
        "        return kl_loss + property_loss + graph_loss + adjacency_loss + features_loss\n",
        "\n",
        "    def _gradient_penalty(self, graph_real, graph_generated):\n",
        "        # Unpack graphs\n",
        "        adjacency_real, features_real = graph_real\n",
        "        adjacency_generated, features_generated = graph_generated\n",
        "\n",
        "        # Generate interpolated graphs (adjacency_interp and features_interp)\n",
        "        alpha = tf.random.uniform([self.batch_size])\n",
        "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1, 1))\n",
        "        adjacency_interp = (adjacency_real * alpha) + (1 - alpha) * adjacency_generated\n",
        "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1))\n",
        "        features_interp = (features_real * alpha) + (1 - alpha) * features_generated\n",
        "\n",
        "        # Compute the logits of interpolated graphs\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(adjacency_interp)\n",
        "            tape.watch(features_interp)\n",
        "            _, _, logits, _, _ = self(\n",
        "                [adjacency_interp, features_interp], training=True\n",
        "            )\n",
        "\n",
        "        # Compute the gradients with respect to the interpolated graphs\n",
        "        grads = tape.gradient(logits, [adjacency_interp, features_interp])\n",
        "        # Compute the gradient penalty\n",
        "        grads_adjacency_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2\n",
        "        grads_features_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2\n",
        "        return tf.reduce_mean(\n",
        "            tf.reduce_mean(grads_adjacency_penalty, axis=(-2, -1))\n",
        "            + tf.reduce_mean(grads_features_penalty, axis=(-1))\n",
        "        )\n",
        "\n",
        "    def inference(self, batch_size):\n",
        "        z = tf.random.normal((batch_size, LATENT_DIM))\n",
        "        reconstruction_adjacency, reconstruction_features = model.decoder.predict(z)\n",
        "        # obtain one-hot encoded adjacency tensor\n",
        "        adjacency = tf.argmax(reconstruction_adjacency, axis=1)\n",
        "        adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)\n",
        "        # Remove potential self-loops from adjacency\n",
        "        adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))\n",
        "        # obtain one-hot encoded feature tensor\n",
        "        features = tf.argmax(reconstruction_features, axis=2)\n",
        "        features = tf.one_hot(features, depth=ATOM_DIM, axis=2)\n",
        "        return [\n",
        "            graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n",
        "            for i in range(batch_size)\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, log_var = self.encoder(inputs)\n",
        "        z = Sampling()([z_mean, log_var])\n",
        "\n",
        "        gen_adjacency, gen_features = self.decoder(z)\n",
        "\n",
        "        property_pred = self.property_prediction_layer(z_mean)\n",
        "\n",
        "        return z_mean, log_var, property_pred, gen_adjacency, gen_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyQcVVodlVKL"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5f7P9DBlVKL"
      },
      "outputs": [],
      "source": [
        "vae_optimizer = tf.keras.optimizers.Adam(learning_rate=VAE_LR)\n",
        "\n",
        "encoder = get_encoder(\n",
        "    gconv_units=[9],\n",
        "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
        "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
        "    latent_dim=LATENT_DIM,\n",
        "    dense_units=[512],\n",
        "    dropout_rate=0.0,\n",
        ")\n",
        "decoder = get_decoder(\n",
        "    dense_units=[128, 256, 512],\n",
        "    dropout_rate=0.2,\n",
        "    latent_dim=LATENT_DIM,\n",
        "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
        "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
        ")\n",
        "\n",
        "model = MoleculeGenerator(encoder, decoder, MAX_MOLSIZE)\n",
        "\n",
        "model.compile(vae_optimizer)\n",
        "history = model.fit([adjacency_tensor, feature_tensor, qed_tensor], epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357qPCRMlVKL"
      },
      "source": [
        "## Inference\n",
        "\n",
        "We use our model to generate new valid molecules from different points of the latent space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tOWup88lVKL"
      },
      "source": [
        "### Generate unique Molecules with the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYLGZ0MYlVKL"
      },
      "outputs": [],
      "source": [
        "molecules = model.inference(1000)\n",
        "\n",
        "MolsToGridImage(\n",
        "    [m for m in molecules if m is not None][:1000], molsPerRow=5, subImgSize=(260, 160)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcUPjHPTlVKM"
      },
      "source": [
        "### Display latent space clusters with respect to molecular properties (QAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPjBVK0vlVKM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_latent(vae, data, labels):\n",
        "    # display a 2D plot of the property in the latent space\n",
        "    z_mean, _ = vae.encoder.predict(data)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_latent(model, [adjacency_tensor[:8000], feature_tensor[:8000]], qed_tensor[:8000])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "molecule_generation",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}